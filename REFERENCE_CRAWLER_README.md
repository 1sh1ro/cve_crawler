# ğŸ”— CVEå‚è€ƒé“¾æ¥çˆ¬è™«

## âœ¨ æ–°åŠŸèƒ½ä»‹ç»

è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ‰©å±•åŠŸèƒ½ï¼Œå¯ä»¥ä»CVE JSONæ–‡ä»¶ä¸­æå–æ‰€æœ‰å‚è€ƒé“¾æ¥ï¼Œè‡ªåŠ¨çˆ¬å–è¿™äº›é“¾æ¥çš„å†…å®¹ï¼Œå¹¶ä¿å­˜åˆ°SQLiteæ•°æ®åº“ä¸­ï¼Œæ–¹ä¾¿åç»­åˆ†æå’ŒæŸ¥è¯¢ã€‚

---

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### 1. è‡ªåŠ¨æå–å‚è€ƒé“¾æ¥
- ä»CVE JSONæ–‡ä»¶ä¸­æå–æ‰€æœ‰å‚è€ƒé“¾æ¥
- è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤çˆ¬å–
- æ”¯æŒæ‰¹é‡å¤„ç†

### 2. æ™ºèƒ½å†…å®¹çˆ¬å–
- è‡ªåŠ¨æå–ç½‘é¡µæ ‡é¢˜å’Œæ­£æ–‡å†…å®¹
- ç§»é™¤JavaScriptå’ŒCSSï¼Œåªä¿ç•™æœ‰ç”¨ä¿¡æ¯
- è®°å½•HTTPçŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯
- æ”¯æŒè‡ªå®šä¹‰è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è¢«å°

### 3. æ•°æ®åº“å­˜å‚¨
- ä½¿ç”¨SQLiteæ•°æ®åº“å­˜å‚¨æ‰€æœ‰æ•°æ®
- CVEä¿¡æ¯å’Œå‚è€ƒé“¾æ¥åˆ†è¡¨å­˜å‚¨
- æ”¯æŒé‡å¤è¿è¡Œï¼Œè‡ªåŠ¨æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
- å®Œæ•´çš„å¤–é”®å…³è”

### 4. å¼ºå¤§çš„æŸ¥è¯¢åŠŸèƒ½
- ç»Ÿè®¡ä¿¡æ¯æŸ¥çœ‹
- æŒ‰CVE IDæŸ¥è¯¢
- æŒ‰åŸŸåæœç´¢
- æŸ¥çœ‹çˆ¬å–çš„å†…å®¹
- å¯¼å‡ºåˆ°CSV

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–
```bash
pip install beautifulsoup4 requests tabulate lxml
```

æˆ–è€…è¿è¡Œï¼š
```bash
install_crawler_deps.bat
```

### åŸºæœ¬ä½¿ç”¨

#### 1. çˆ¬å–å‚è€ƒé“¾æ¥ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
```bash
python reference_crawler.py linux_kernel_privilege_escalation_2020-2025.json --max 5
```

#### 2. çˆ¬å–å…¨éƒ¨æ•°æ®
```bash
python reference_crawler.py linux_kernel_privilege_escalation_2020-2025.json
```

#### 3. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
```bash
python query_references.py stats
```

#### 4. æŸ¥è¯¢ç‰¹å®šCVE
```bash
python query_references.py cve CVE-2020-25221
```

#### 5. æœç´¢ç‰¹å®šåŸŸå
```bash
python query_references.py domain intel.com
```

#### 6. æŸ¥çœ‹å†…å®¹
```bash
python query_references.py content CVE-2020-25221 --url intel.com
```

---

## ğŸ“Š æµ‹è¯•ç»“æœ

### æµ‹è¯•æ•°æ®ï¼ˆå‰3ä¸ªCVEï¼‰
```
æ€»CVEæ•°: 3
æ€»é“¾æ¥æ•°: 15
æˆåŠŸçˆ¬å–: 13 (86.7%)
å¤±è´¥æ•°é‡: 2

å‰10ä¸ªåŸŸå:
+---------------------+------+
| åŸŸå                  | æ•°é‡ |
+=====================+======+
| www.openwall.com    |    3 |
| git.kernel.org      |    3 |
| security.netapp.com |    2 |
| lists.opensuse.org  |    2 |
| lists.debian.org    |    2 |
| bugzilla.redhat.com |    2 |
| cdn.kernel.org      |    1 |
+---------------------+------+
```

### æˆåŠŸçˆ¬å–çš„å†…å®¹ç¤ºä¾‹
- âœ… Openwallé‚®ä»¶åˆ—è¡¨å½’æ¡£
- âœ… Git kernelæäº¤è®°å½•
- âœ… Debianå®‰å…¨å…¬å‘Š
- âœ… Red Hat Bugzilla
- âœ… NetAppå®‰å…¨å…¬å‘Š
- âš ï¸ éƒ¨åˆ†ç½‘ç«™éœ€è¦JavaScriptï¼ˆå¦‚NetAppï¼‰

---

## ğŸ’¡ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: æ·±åº¦åˆ†æç‰¹å®šæ¼æ´
```bash
# 1. çˆ¬å–æ•°æ®
python reference_crawler.py linux_kernel_privilege_escalation_2020-2025.json

# 2. æŸ¥è¯¢CVEè¯¦æƒ…
python query_references.py cve CVE-2023-2163

# 3. æŸ¥çœ‹æ‰€æœ‰å‚è€ƒé“¾æ¥çš„å†…å®¹
python query_references.py content CVE-2023-2163
```

### åœºæ™¯2: åˆ†æå‚å•†å®‰å…¨å“åº”
```bash
# æŸ¥è¯¢Intelçš„æ‰€æœ‰å®‰å…¨å…¬å‘Š
python query_references.py domain intel.com

# æŸ¥è¯¢Red Hatçš„æ‰€æœ‰Bugzilla
python query_references.py domain bugzilla.redhat.com

# å¯¼å‡ºåˆ†æ
python query_references.py export --output vendor_analysis.csv
```

### åœºæ™¯3: è¿½è¸ªè¡¥ä¸ä¿¡æ¯
```bash
# æŸ¥è¯¢æ‰€æœ‰kernel.orgçš„é“¾æ¥
python query_references.py domain kernel.org

# æŸ¥çœ‹å…·ä½“è¡¥ä¸å†…å®¹
python query_references.py content CVE-2020-25221 --url kernel.org
```

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒæ–‡ä»¶
- `reference_crawler.py` - å‚è€ƒé“¾æ¥çˆ¬è™«ä¸»ç¨‹åº
- `query_references.py` - æ•°æ®åº“æŸ¥è¯¢å·¥å…·
- `cve_references.db` - SQLiteæ•°æ®åº“ï¼ˆè¿è¡Œåç”Ÿæˆï¼‰

### æ–‡æ¡£æ–‡ä»¶
- `REFERENCE_CRAWLER_GUIDE.md` - è¯¦ç»†ä½¿ç”¨æŒ‡å—
- `REFERENCE_CRAWLER_README.md` - æœ¬æ–‡ä»¶

### è¾…åŠ©æ–‡ä»¶
- `install_crawler_deps.bat` - ä¾èµ–å®‰è£…è„šæœ¬

---

## ğŸ—„ï¸ æ•°æ®åº“ç»“æ„

### CVEè¡¨ (cves)
```sql
CREATE TABLE cves (
    id INTEGER PRIMARY KEY,
    cve_id TEXT UNIQUE,
    description TEXT,
    cvss_score REAL,
    severity TEXT,
    published_date TEXT,
    created_at TIMESTAMP
)
```

### å‚è€ƒé“¾æ¥è¡¨ (reference_links)
```sql
CREATE TABLE reference_links (
    id INTEGER PRIMARY KEY,
    cve_id TEXT,
    url TEXT,
    domain TEXT,
    title TEXT,
    content TEXT,
    status_code INTEGER,
    crawled_at TIMESTAMP,
    error TEXT,
    UNIQUE(cve_id, url)
)
```

---

## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°

### reference_crawler.py
```bash
python reference_crawler.py <json_file> [é€‰é¡¹]

é€‰é¡¹:
  --db DB          æ•°æ®åº“æ–‡ä»¶åï¼ˆé»˜è®¤: cve_references.dbï¼‰
  --max MAX        æœ€å¤§å¤„ç†CVEæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
  --delay DELAY    è¯·æ±‚å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 1ï¼‰
```

### query_references.py
```bash
python query_references.py [å‘½ä»¤] [é€‰é¡¹]

å‘½ä»¤:
  stats                    æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  cve <cve_id>            æŸ¥è¯¢ç‰¹å®šCVE
  domain <domain>         æœç´¢ç‰¹å®šåŸŸå
  content <cve_id>        æŸ¥çœ‹å†…å®¹
  export                  å¯¼å‡ºåˆ°CSV

é€‰é¡¹:
  --db DB                 æ•°æ®åº“æ–‡ä»¶å
  --url URL               URLæ¨¡å¼ï¼ˆç”¨äºcontentå‘½ä»¤ï¼‰
  --output OUTPUT         è¾“å‡ºæ–‡ä»¶åï¼ˆç”¨äºexportå‘½ä»¤ï¼‰
```

---

## ğŸ“ é«˜çº§æŠ€å·§

### 1. å¹¶å‘çˆ¬å–ï¼ˆæé«˜é€Ÿåº¦ï¼‰
ä¿®æ”¹ `reference_crawler.py`ï¼Œä½¿ç”¨çº¿ç¨‹æ± ï¼š
```python
from concurrent.futures import ThreadPoolExecutor

# åœ¨process_json_fileæ–¹æ³•ä¸­ä½¿ç”¨
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(self.crawl_url, url) for url in references]
    results = [f.result() for f in futures]
```

### 2. ä½¿ç”¨ä»£ç†
```python
# åœ¨ReferenceCrawler.__init__ä¸­æ·»åŠ 
self.session.proxies = {
    'http': 'http://proxy:port',
    'https': 'https://proxy:port'
}
```

### 3. è‡ªå®šä¹‰User-Agent
```python
# ä¿®æ”¹session.headers
self.session.headers.update({
    'User-Agent': 'Your Custom User Agent'
})
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. çˆ¬å–é€Ÿåº¦
- é»˜è®¤å»¶è¿Ÿ1ç§’ï¼Œé¿å…è¢«å°
- å»ºè®®å…ˆç”¨ `--max 5` æµ‹è¯•
- æŸäº›ç½‘ç«™å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶

### 2. å†…å®¹é™åˆ¶
- æ¯ä¸ªé¡µé¢å†…å®¹é™åˆ¶10000å­—ç¬¦
- åŠ¨æ€åŠ è½½çš„å†…å®¹å¯èƒ½æ— æ³•è·å–
- JavaScriptæ¸²æŸ“çš„é¡µé¢éœ€è¦ç‰¹æ®Šå¤„ç†

### 3. ç½‘ç»œé—®é¢˜
- è¶…æ—¶è®¾ç½®ä¸º10ç§’
- å¤±è´¥çš„é“¾æ¥ä¼šè®°å½•é”™è¯¯ä¿¡æ¯
- å¯ä»¥é‡æ–°è¿è¡Œæ›´æ–°å¤±è´¥çš„é“¾æ¥

### 4. æ³•å¾‹åˆè§„
- éµå®ˆrobots.txt
- å°Šé‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾
- ä¸è¦è¿‡åº¦é¢‘ç¹è¯·æ±‚

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: ä¾èµ–ç¼ºå¤±
```bash
pip install beautifulsoup4 requests tabulate lxml
```

### é—®é¢˜2: æ•°æ®åº“é”å®š
```bash
# å…³é—­æ‰€æœ‰è®¿é—®æ•°æ®åº“çš„ç¨‹åº
# æˆ–ä½¿ç”¨æ–°æ•°æ®åº“
python reference_crawler.py data.json --db new_db.db
```

### é—®é¢˜3: ç¼–ç é”™è¯¯
ç¡®ä¿JSONæ–‡ä»¶æ˜¯UTF-8ç¼–ç 

### é—®é¢˜4: è¿æ¥è¶…æ—¶
```bash
# å¢åŠ å»¶è¿Ÿæ—¶é—´
python reference_crawler.py data.json --delay 3
```

---

## ğŸ“ˆ æ€§èƒ½æ•°æ®

### çˆ¬å–é€Ÿåº¦
- æ— APIé™åˆ¶: çº¦1ä¸ªé“¾æ¥/ç§’
- æœ‰å»¶è¿Ÿä¿æŠ¤: çº¦0.5-2ä¸ªé“¾æ¥/ç§’
- 96ä¸ªCVEï¼Œçº¦450ä¸ªé“¾æ¥: çº¦7-15åˆ†é’Ÿ

### æ•°æ®åº“å¤§å°
- 100ä¸ªCVE: çº¦5-10MB
- 1000ä¸ªCVE: çº¦50-100MB
- å–å†³äºå†…å®¹é•¿åº¦

---

## ğŸ‰ æ€»ç»“

è¿™ä¸ªå‚è€ƒé“¾æ¥çˆ¬è™«ä¸ºCVEåˆ†ææä¾›äº†å¼ºå¤§çš„æ•°æ®æ”¶é›†èƒ½åŠ›ï¼š

âœ… **è‡ªåŠ¨åŒ–** - ä¸€é”®çˆ¬å–æ‰€æœ‰å‚è€ƒé“¾æ¥
âœ… **ç»“æ„åŒ–** - æ•°æ®å­˜å‚¨åœ¨SQLiteæ•°æ®åº“
âœ… **å¯æŸ¥è¯¢** - å¼ºå¤§çš„æŸ¥è¯¢å’Œå¯¼å‡ºåŠŸèƒ½
âœ… **å¯æ‰©å±•** - æ˜“äºæ·»åŠ æ–°åŠŸèƒ½
âœ… **å®ç”¨æ€§** - çœŸå®çš„å®‰å…¨ç ”ç©¶åœºæ™¯

é…åˆCVEçˆ¬è™«ä½¿ç”¨ï¼Œå¯ä»¥æ„å»ºå®Œæ•´çš„æ¼æ´æƒ…æŠ¥æ”¶é›†ç³»ç»Ÿï¼

---

**éœ€è¦å¸®åŠ©ï¼ŸæŸ¥çœ‹ `REFERENCE_CRAWLER_GUIDE.md` è·å–è¯¦ç»†æ–‡æ¡£ï¼** ğŸ“–
